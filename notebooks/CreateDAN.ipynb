{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating own DAN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "# from keras.layers import Layer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from typing import *\n",
    "import pickle\n",
    "import random\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../datasets/cleaned_datasets/filtered_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(_dir:str) -> pd.DataFrame:\n",
    "    data = {}\n",
    "    #data[\"url\"] = []\n",
    "    data[\"text\"] = []\n",
    "    data[\"class\"] = []\n",
    "    for root, dirs, files in os.walk(_dir):\n",
    "        for _dir in dirs: \n",
    "            for txt_file in [x for x in os.listdir(os.path.join(root, _dir)) if x.endswith((\".txt\", \".TXT\"))]:\n",
    "                # Class name = dir name\n",
    "                class_name = _dir\n",
    "                #Read File\n",
    "                file_name = os.path.abspath(os.path.join(root, _dir, txt_file))\n",
    "                file = open(file_name, \"r\")\n",
    "                txt = file.read()\n",
    "                file.close()\n",
    "                #data[\"url\"].append(file_name)\n",
    "                data[\"text\"].append(txt)\n",
    "                data[\"class\"].append(class_name)\n",
    "    df = pd.DataFrame.from_dict(data)\n",
    "    del data\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_data(data_dir).sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5811</th>\n",
       "      <td>like traditional association rule mining the d...</td>\n",
       "      <td>pattern_mining</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8439</th>\n",
       "      <td>introduction association rule mining and class...</td>\n",
       "      <td>pattern_mining</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6365</th>\n",
       "      <td>discovered association rules among web objects...</td>\n",
       "      <td>pattern_mining</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10149</th>\n",
       "      <td>sequential methods many sequential methods hav...</td>\n",
       "      <td>pattern_mining</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6310</th>\n",
       "      <td>in the area of association rule mining most pr...</td>\n",
       "      <td>pattern_mining</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text           class\n",
       "5811   like traditional association rule mining the d...  pattern_mining\n",
       "8439   introduction association rule mining and class...  pattern_mining\n",
       "6365   discovered association rules among web objects...  pattern_mining\n",
       "10149  sequential methods many sequential methods hav...  pattern_mining\n",
       "6310   in the area of association rule mining most pr...  pattern_mining"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating One-Hot-Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11874, 1)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop([\"text\"], axis = 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# Transform classes into dummies\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "classes = df.drop([\"text\"], axis = 1)\n",
    "classes.apply(le.fit_transform)\n",
    "\n",
    "# Create One Hot Encodings\n",
    "enc = preprocessing.OneHotEncoder()\n",
    "enc.fit(classes)\n",
    "\n",
    "\n",
    "one_hot_encodings = enc.transform(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encodings = one_hot_encodings.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11874, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastText():\n",
    "    \"\"\"\n",
    "    Loads the FastText model and get the Vectors.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        # super().__init__(**kwargs)\n",
    "        self.path = \"/Users/Daniel/PycharmProjects/Recommender-System/notebooks/FastText/ft_model_15000.pkl\"\n",
    "        self.__initialize_model()\n",
    "\n",
    "    def __initialize_model(self, **kwargs):\n",
    "        try:\n",
    "            tf.logging.info(\"FastText Model is loading\")\n",
    "            self.model = pickle.load(open(self.path, \"rb\"))\n",
    "            tf.logging.info(\"FastText Model loaded!\")\n",
    "        except Exception as e:\n",
    "            tf.logging.warning(\"Something went wrong while loading the FastText Model..\")\n",
    "            tf.logging.warning(e)\n",
    "\n",
    "    def inference(self, words: List[str]) -> np.ndarray:\n",
    "        embeddings = []\n",
    "        for word in words:\n",
    "            if self.model.wv.__contains__(word):\n",
    "                embeddings.append(self.model.wv.__getitem__(word))\n",
    "        return np.array(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:FastText Model is loading\n",
      "INFO:tensorflow:FastText Model loaded!\n"
     ]
    }
   ],
   "source": [
    "ft_path = \"/Users/Daniel/PycharmProjects/Recommender-System/notebooks/FastText/ft_model_15000.pkl\"\n",
    "fast_text_model = FastText()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input = tf.placeholder(dtype = tf.float64, shape= (None, 100), name = \"placeholder_input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = tf.placeholder(dtype = tf.float64, shape = (None, 3), name = \"placeholder_y_true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Costumized Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_layer(input_layer:np.ndarray ,dropout_prob:float = 0.2):\n",
    "    #dist = tf.contrib.distributions.Binomial(1, self.drop)\n",
    "    dist = 1 - np.random.binomial(1, dropout_prob, input_layer.shape[0])\n",
    "    return input_layer[dist.astype(bool)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_layer(placeholder: tf.Variable, name= \"average_layer\"):\n",
    "    with tf.name_scope(name):\n",
    "        average = tf.reduce_mean(input_tensor=placeholder, axis=0)\n",
    "        return tf.reshape(average, (1,-1), name = \"average_layer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "average = average_layer(x_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Dense Layers for DAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layer_1 = tf.layers.dense(inputs=average,\n",
    "                                      units=100,\n",
    "                                      activation=tf.nn.tanh,\n",
    "                                      use_bias=True,\n",
    "                                      trainable=True,\n",
    "                                      name=\"dense_layer_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layer_2 = tf.layers.dense(inputs=dense_layer_1,\n",
    "                                      units=100,\n",
    "                                      activation=tf.nn.tanh,\n",
    "                                      use_bias=True,\n",
    "                                      trainable=True,\n",
    "                                      name=\"dense_layer_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layer_3 = tf.layers.dense(inputs=dense_layer_2,\n",
    "                                      units=100,\n",
    "                                      activation=tf.nn.tanh,\n",
    "                                      use_bias=True,\n",
    "                                      trainable=True,\n",
    "                                      name=\"dense_layer_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layer_4 = tf.layers.dense(inputs=dense_layer_3,\n",
    "                                      units=100,\n",
    "                                      activation=tf.nn.tanh,\n",
    "                                      use_bias=True,\n",
    "                                      trainable=True,\n",
    "                                      name=\"dense_layer_4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Dense Layers for Clasification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layer_5 = tf.layers.dense(inputs=dense_layer_4,\n",
    "                                      units=200,\n",
    "                                      activation=tf.nn.tanh,\n",
    "                                      use_bias=True,\n",
    "                                      trainable=True,\n",
    "                                      name=\"dense_layer_5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = tf.layers.dense(inputs=dense_layer_5,\n",
    "                                      units=3,\n",
    "                                      activation=tf.nn.tanh,\n",
    "                                      use_bias=True,\n",
    "                                      trainable=True,\n",
    "                                      name=\"logits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Layer & Loss Function\n",
    "https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    softmax_layer = tf.nn.softmax_cross_entropy_with_logits_v2(labels = y_true, logits=logits) # WTF Is this function actually? \n",
    "    cross_entropy = tf.reduce_mean(softmax_layer)\n",
    "    tf.summary.scalar(\"cross_entropy\", cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-4)\n",
    "    train = optimizer.minimize(cross_entropy)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics - Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"pred_eval\"):\n",
    "    prob_target = tf.argmax(logits, 1, name=\"prob_target\") # Returns Index with largest value\n",
    "    prob_true = tf.argmax(y_true, 1, name = \"prob_true\")\n",
    "    pred_eval = tf.equal(prob_target, prob_true)\n",
    "    \n",
    "    #_sum = tf.add(predictions, tf.cast(correct_prediction, dtype=tf.float64))\n",
    "    #correct_predictions = tf.concat(prob_target, prob_true)\n",
    "    #correct_pred = tf.cast(correct_predictions, dtype=tf.int32)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y_true))\n",
    "    #print(\"correct_prediction: {}\".format(correct_prediction))\n",
    "    \n",
    "    #accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    #tf.summary.scalar(\"accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"dan_checkpoints/\" # Need to exist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"first_dan.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the Graph\n",
    "if not os.path.isdir(save_path):\n",
    "    os.makedirs(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 Epoches rund 6 Minuten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9414687552636012\n",
      "INFO:tensorflow:Model saved.\n",
      "CPU times: user 31.3 s, sys: 4.27 s, total: 35.6 s\n",
      "Wall time: 20.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with tf.Session() as sess: \n",
    "    sess.run(init)\n",
    "    \n",
    "    ### Tensorboard Stuff\n",
    "    ## TODO at summary\n",
    "    #merged_summary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(save_path)\n",
    "    writer.add_graph(sess.graph)\n",
    "\n",
    "    epoches = 1\n",
    "    \n",
    "    preds = []\n",
    "    for i in range(epoches):\n",
    "        for j,p in enumerate([word_tokenize(x) for x in df[\"text\"].tolist()]): \n",
    "            # Converting paragraphs to Embeddings\n",
    "            embeddings = dropout_layer(fast_text_model.inference(p))\n",
    "            label = np.array(one_hot_encodings[j]).reshape(1,-1)\n",
    "            \n",
    "            \n",
    "            [_, _eval] = sess.run([train, pred_eval], feed_dict={x_input:embeddings, y_true:label})\n",
    "            preds.append(_eval[0])\n",
    "        print(\"Accuracy: {}\".format(np.mean(preds)))\n",
    "    saver.save(sess, os.path.join(save_path, model_name))\n",
    "    \n",
    "    \n",
    "    # Save graph definition\n",
    "    graph = sess.graph\n",
    "    graph_def = graph.as_graph_def()\n",
    "    tf.train.write_graph(graph_def, \".\", \"dan_checkpoints/first_dan.pbtxt\", True)\n",
    "    \n",
    "    tf.logging.info(\"Model saved.\")\n",
    "    \n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dense_layer_4/Tanh:0'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_layer_4.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from dan_checkpoints/first_dan.ckpt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "run() missing 1 required positional argument: 'fetches'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-3ec67b981437>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdan_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tensor_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dense_layer_4/Tanh:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdan_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: run() missing 1 required positional argument: 'fetches'"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess: \n",
    "    saver.restore(sess,os.path.join(save_path, model_name))\n",
    "    dan_tensor = tf.get_default_graph().get_tensor_by_name(\"dense_layer_4/Tanh:0\")\n",
    "    sess.run()\n",
    "    print(dan_tensor)\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dan_embedding(text: List[str], layer_name:str):\n",
    "    with tf.Session() as sess: \n",
    "        saver.restore(sess,os.path.join(save_path, model_name))\n",
    "        \n",
    "        embeddings = fast_text_model.inference(text)\n",
    "        t_name = layer_name+\"/Tanh:0\"\n",
    "        dan_layer = tf.get_default_graph().get_tensor_by_name(t_name)\n",
    "        dan_emb = sess.run(dan_layer, feed_dict={x_input:embeddings})\n",
    "        \n",
    "        sess.close()\n",
    "        return dan_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_dan_embedding([\"hello\", \"my\"], \"dense_layer_4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Embeddings for Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_1 = \"i want predict cost\"\n",
    "example_2 = \"i want predict stock prize\"\n",
    "example_3 = \"i want similar cost\"\n",
    "example_4 = \"i want pattern in my costs\"\n",
    "\n",
    "examples = [example_1, example_2, example_3, example_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastText_embeddings = [np.mean(fast_text_model.inference(word_tokenize(x)), axis = 0) for x in examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dan_embeddings = np.squeeze([get_dan_embedding(word_tokenize(x), layer = dense_layer_4) for x in examples]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = pd.DataFrame.from_dict({\"text\": examples, \"ft_embedding\": fastText_embeddings, \"dan_embedding\": dan_embeddings})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_FT = PCA(n_components=2)\n",
    "pca_DAN = PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_DAN = pca_FT.fit_transform(_df[\"dan_embedding\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_FT = pca_FT.fit_transform(_df[\"ft_embedding\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df[\"ft_x\"] = matrix_FT[:,0]\n",
    "_df[\"ft_y\"] = matrix_FT[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df[\"dan_x\"] = matrix_DAN[:,0]\n",
    "_df[\"dan_y\"] = matrix_DAN[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(x = \"ft_x\", y=\"ft_y\", data = _df, s = 100,  hue = \"text\")\n",
    "title = ax.set_title(\"FastText\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(x = \"dan_x\", y=\"dan_y\", data = _df, s=100, hue = \"text\")\n",
    "title = ax.set_title(\"DAN\")\n",
    "ax.set(ylim=(-5, 5))\n",
    "ax.set(xlim=(-5, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pertubation\n",
    "As used in:  http://cs.umd.edu/~miyyer/pubs/2015_acl_dan.pdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [dense_layer_1, dense_layer_2, dense_layer_3, dense_layer_4]\n",
    "for i,layer in enumerate(layers): \n",
    "    _df[\"layer_{}\".format(i)] = _df[\"text\"].apply(lambda text: np.linalg.norm(get_dan_embedding(text, layers[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(layers)):\n",
    "    x = [x+1 for x in range(0,len(layers))]\n",
    "    y = _df.drop(columns = [\"text\",\"ft_embedding\", \"dan_embedding\", \"ft_x\", \"ft_y\", \"dan_x\", \"dan_y\"]).T.values[:,i]\n",
    "    print(y)\n",
    "    ax = sns.lineplot(x = range(1, len(layers)+1), y = y, markers = \"x\")\n",
    "    #ax.set(xlim = (0,len(layers)+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Density Plots on Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.drop(columns = [\"x\",\"y\",\"z\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "df[\"embedding\"] = df[\"text\"].apply(lambda x: get_dan_embedding(word_tokenize(x), layers[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "matrix = pca.fit_transform(df[\"embedding\"].apply(lambda x: x[0]).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"x\"] = matrix[:,0]\n",
    "df[\"y\"] = matrix[:,1]\n",
    "#df[\"z\"] = matrix[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "import matplotlib\n",
    "\n",
    "cmaps = cmaps = [\"Reds\",\"YlOrBr\", \"Blues\",\"Greens\"]\n",
    "#colors = [\"red\", \"blue\", \"purple\", \"green\", \"\", \"grey\"]\n",
    "groups = df.groupby(\"class\")\n",
    "counter = 0\n",
    "\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "fig.suptitle(\"Density & Scatter Plot\")\n",
    "\n",
    "ax_1 = fig.add_subplot(1,2,1)\n",
    "ax_2 = fig.add_subplot(1,2,2)\n",
    "\n",
    "ax_1.set_title(\"Density Plot\")\n",
    "ax_2.set_title(\"Scatter Plot\")\n",
    "\n",
    "\n",
    "labels = []\n",
    "for name, group in groups: \n",
    "    # Change that ax = ax_1\n",
    "    ax = sns.kdeplot(group.x,group.y, shade=True, cmap = cmaps[counter],shade_lowest=False, ax = ax_1)\n",
    "    color_from_cmap = matplotlib.cm.get_cmap(cmaps[counter])(0.5)\n",
    "    \n",
    "    ax_2.scatter(group.x, group.y, color = color_from_cmap)\n",
    "    labels.append(mpatches.Patch(color=color_from_cmap, label=name))\n",
    "    counter +=1\n",
    "legend = plt.legend(handles = labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serving the model\n",
    "__FUCK IT!__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an Evaluation Framework\n",
    "Goal is to create a class which makes it easy to create a DAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_layer(input_layer:np.ndarray ,dropout_prob:float = 0.2):\n",
    "    #dist = tf.contrib.distributions.Binomial(1, self.drop)\n",
    "    dist = 1 - np.random.binomial(1, dropout_prob, input_layer.shape[0])\n",
    "    return input_layer[dist.astype(bool)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_layer(placeholder: tf.Variable, name= \"average_layer\"):\n",
    "    with tf.name_scope(name):\n",
    "        average = tf.reduce_mean(input_tensor=placeholder, axis=0)\n",
    "        return tf.reshape(average, (1,-1), name = \"average_layer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAN:\n",
    "    def __init__(self, num_dan_layers: int,\n",
    "             shape_classifier_layers: List[int],\n",
    "             word_embedding_model: Any,\n",
    "             activation_func: Any = tf.nn.tanh,\n",
    "             wd_prob: float = 0.2):\n",
    "\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.shape_classifier_layers = shape_classifier_layers\n",
    "        self.activation_func = activation_func\n",
    "        self.embedding_model = word_embedding_model\n",
    "        self.embedding_len = 100  # TODO HARDCODED\n",
    "        self.wd_prob = wd_prob\n",
    "        self.dense_layers = []\n",
    "        \n",
    "        self.train = self.__init_architecture()\n",
    "        \n",
    "    def __init_architecture() -> tf.Tensor:\n",
    "        x_input = tf.placeholder(dtype = tf.float64, shape = (None, 100), name = \"placeholder_input\")\n",
    "        y_true = tf.placeholder(dtype = tf.float64, shape = (None, 3), name = \"placeholder_y_true\")\n",
    "        \n",
    "        # Average Layer\n",
    "        average = average_layer(x_input)\n",
    "        # Maybe a Function\n",
    "        dense_layer_1 = tf.layers.dense(inputs=average,\n",
    "                                      units=100,\n",
    "                                      activation=tf.nn.tanh,\n",
    "                                      use_bias=True,\n",
    "                                      trainable=True,\n",
    "                                      name=\"dense_layer_1\")\n",
    "        \n",
    "        dense_layer_2 = tf.layers.dense(inputs=dense_layer_2,\n",
    "                                      units=100,\n",
    "                                      activation=tf.nn.tanh,\n",
    "                                      use_bias=True,\n",
    "                                      trainable=True,\n",
    "                                      name=\"dense_layer_1\")\n",
    "        dense_layer_5 = tf.layers.dense(inputs=dense_layer_4,\n",
    "                                      units=200,\n",
    "                                      activation=tf.nn.tanh,\n",
    "                                      use_bias=True,\n",
    "                                      trainable=True,\n",
    "                                      name=\"dense_layer_5\")\n",
    "        logits = tf.layers.dense(inputs=dense_layer_5,\n",
    "                                      units=3,\n",
    "                                      activation=tf.nn.tanh,\n",
    "                                      use_bias=True,\n",
    "                                      trainable=True,\n",
    "                                      name=\"logits\")\n",
    "        \n",
    "        with tf.name_scope(\"loss\"):\n",
    "            softmax_layer = tf.nn.softmax_cross_entropy_with_logits_v2(labels = y_true, logits=logits) # WTF Is this function actually? \n",
    "            cross_entropy = tf.reduce_mean(softmax_layer)\n",
    "            tf.summary.scalar(\"cross_entropy\", cross_entropy)\n",
    "        with tf.name_scope(\"train\"):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=1e-4)\n",
    "            train = optimizer.minimize(cross_entropy)\n",
    "        \n",
    "            return train\n",
    "    def train(text: List[str],labels: List[str], save_path: str, model_name:str, epoches:int = 1):\n",
    "        with tf.Session() as sess: \n",
    "            sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "            ### Tensorboard Stuff\n",
    "            ## TODO at summary\n",
    "            #merged_summary = tf.summary.merge_all()\n",
    "            writer = tf.summary.FileWriter(save_path)\n",
    "            writer.add_graph(sess.graph)\n",
    "\n",
    "            epoches = 1\n",
    "\n",
    "            preds = []\n",
    "            for i in range(epoches):\n",
    "                for j,p in enumerate([word_tokenize(x) for x in text]): \n",
    "                    # Converting paragraphs to Embeddings\n",
    "                    embeddings = dropout_layer(fast_text_model.inference(p))\n",
    "                    label = np.array(one_hot_encodings[j]).reshape(1,-1)\n",
    "                    [_, _eval] = sess.run([self.train], feed_dict={x_input:embeddings, y_true:label})\n",
    "            saver.save(sess, os.path.join(save_path, model_name))\n",
    "\n",
    "\n",
    "            # Save graph definition\n",
    "            graph = sess.graph\n",
    "            graph_def = g.as_graph_def()\n",
    "            tf.train.write_graph(graph_def, \".\", \"dan_checkpoints/first_dan.pbtxt\", True)\n",
    "\n",
    "            tf.logging.info(\"Model saved.\")\n",
    "\n",
    "            sess.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
