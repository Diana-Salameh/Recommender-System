{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating own DAN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "# from keras.layers import Layer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from typing import *\n",
    "import pickle\n",
    "import random\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../datasets/cleaned_datasets/filtered_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(_dir:str) -> pd.DataFrame:\n",
    "    data = {}\n",
    "    #data[\"url\"] = []\n",
    "    data[\"text\"] = []\n",
    "    data[\"class\"] = []\n",
    "    for root, dirs, files in os.walk(_dir):\n",
    "        for _dir in dirs: \n",
    "            for txt_file in [x for x in os.listdir(os.path.join(root, _dir)) if x.endswith((\".txt\", \".TXT\"))]:\n",
    "                # Class name = dir name\n",
    "                class_name = _dir\n",
    "                #Read File\n",
    "                file_name = os.path.abspath(os.path.join(root, _dir, txt_file))\n",
    "                file = open(file_name, \"r\")\n",
    "                txt = file.read()\n",
    "                file.close()\n",
    "                #data[\"url\"].append(file_name)\n",
    "                data[\"text\"].append(txt)\n",
    "                data[\"class\"].append(class_name)\n",
    "    df = pd.DataFrame.from_dict(data)\n",
    "    del data\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_data(data_dir).sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>a framework of multiclass sentiment classifica...</td>\n",
       "      <td>prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3345</th>\n",
       "      <td>the kmedoids clustering is very similar to kme...</td>\n",
       "      <td>clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7419</th>\n",
       "      <td>for example positive association rule high fre...</td>\n",
       "      <td>pattern_mining</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9687</th>\n",
       "      <td>effective malicious sequential pattern mining ...</td>\n",
       "      <td>pattern_mining</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1146</th>\n",
       "      <td>due to the large number of possible label sets...</td>\n",
       "      <td>prediction</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text           class\n",
       "537   a framework of multiclass sentiment classifica...      prediction\n",
       "3345  the kmedoids clustering is very similar to kme...      clustering\n",
       "7419  for example positive association rule high fre...  pattern_mining\n",
       "9687  effective malicious sequential pattern mining ...  pattern_mining\n",
       "1146  due to the large number of possible label sets...      prediction"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.get_dummies(df[\"class\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating One-Hot-Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# Transform classes into dummies\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "classes = df.drop([\"text\"], axis = 1)\n",
    "classes.apply(le.fit_transform)\n",
    "\n",
    "# Create One Hot Encodings\n",
    "enc = preprocessing.OneHotEncoder()\n",
    "enc.fit(classes)\n",
    "\n",
    "\n",
    "one_hot_encodings = enc.transform(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11874, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encodings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11874, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encodings.toarray()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastText():\n",
    "    \"\"\"\n",
    "    Loads the FastText model and get the Vectors.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        # super().__init__(**kwargs)\n",
    "        self.path = \"/Users/Daniel/PycharmProjects/Recommender-System/notebooks/FastText/ft_model_15000.pkl\"\n",
    "        self.__initialize_model()\n",
    "\n",
    "    def __initialize_model(self, **kwargs):\n",
    "        try:\n",
    "            tf.logging.info(\"FastText Model is loading\")\n",
    "            self.model = pickle.load(open(self.path, \"rb\"))\n",
    "            tf.logging.info(\"FastText Model loaded!\")\n",
    "        except Exception as e:\n",
    "            tf.logging.warning(\"Something went wrong while loading the FastText Model..\")\n",
    "            tf.logging.warning(e)\n",
    "\n",
    "    def inference(self, words: List[str]) -> np.ndarray:\n",
    "        embeddings = []\n",
    "        for word in words:\n",
    "            if self.model.wv.__contains__(word):\n",
    "                embeddings.append(self.model.wv.__getitem__(word))\n",
    "        return np.array(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:FastText Model is loading\n",
      "INFO:tensorflow:FastText Model loaded!\n"
     ]
    }
   ],
   "source": [
    "ft_path = \"/Users/Daniel/PycharmProjects/Recommender-System/notebooks/FastText/ft_model_15000.pkl\"\n",
    "fast_text_model = FastText()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Layer\n",
    "We need: \n",
    "    1. Input Layer with Dropout\n",
    "    2. Average Layer\n",
    "    3. Dense Layer\n",
    "    4. Dense Layer (Classifier)\n",
    "    5. Output Softmax Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = [[\"this\", \"sucks\"], [\"hello\", \"my\", \"name\", \"is\", \"daniel\"],[\"clustering\", \"is\", \"amazing\"]]\n",
    "paragraphs_y = np.asarray([[0,1,0], [0,0,1], [1,0,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = fast_text_model.inference(paragraphs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 100)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_layer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input = tf.placeholder(dtype = tf.float64, shape = (None, 100), name = \"placeholder_input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = tf.placeholder(dtype = tf.float64, shape = (None, 3), name = \"placeholder_y_true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Costumized Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_layer(input_layer:np.ndarray ,dropout_prob:float = 0.2):\n",
    "    #dist = tf.contrib.distributions.Binomial(1, self.drop)\n",
    "    dist = 1 - np.random.binomial(1, dropout_prob, input_layer.shape[0])\n",
    "    return input_layer[dist.astype(bool)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = dropout_layer(input_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_layer(dropout_layer:np.ndarray, placeholder: tf.placeholder,name= \"average_layer\"):\n",
    "    with tf.name_scope(name):\n",
    "        average = tf.reduce_mean(input_tensor=placeholder, axis=0)\n",
    "        return tf.reshape(average, (1,-1), name = \"average_layer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "average = average_layer(dl, x_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Dense Layers for DAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layer_1 = tf.layers.dense(inputs=average,\n",
    "                                      units=100,\n",
    "                                      activation=tf.nn.tanh,\n",
    "                                      use_bias=True,\n",
    "                                      trainable=True,\n",
    "                                      name=\"dense_layer_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layer_2 = tf.layers.dense(inputs=dense_layer_1,\n",
    "                                      units=100,\n",
    "                                      activation=tf.nn.tanh,\n",
    "                                      use_bias=True,\n",
    "                                      trainable=True,\n",
    "                                      name=\"dense_layer_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Dense Layers for Clasification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layer_3 = tf.layers.dense(inputs=dense_layer_2,\n",
    "                                      units=200,\n",
    "                                      activation=tf.nn.tanh,\n",
    "                                      use_bias=True,\n",
    "                                      trainable=True,\n",
    "                                      name=\"dense_layer_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = tf.layers.dense(inputs=dense_layer_3,\n",
    "                                      units=3,\n",
    "                                      activation=tf.nn.tanh,\n",
    "                                      use_bias=True,\n",
    "                                      trainable=True,\n",
    "                                      name=\"logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Layer & Loss Function\n",
    "https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    softmax_layer = tf.nn.softmax_cross_entropy_with_logits_v2(labels = y_true, logits=logits) # WTF Is this function actually? \n",
    "    cross_entropy = tf.reduce_mean(softmax_layer)\n",
    "    tf.summary.scalar(\"cross_entropy\", cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-4)\n",
    "    train = optimizer.minimize(cross_entropy)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics - Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y_true))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"dan_checkpoints/\" # Need to exist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"first_dan.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the Graph\n",
    "if not os.path.isdir(save_path):\n",
    "    os.makedirs(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with tf.Session() as sess: \n",
    "    sess.run(init)\n",
    "    \n",
    "    ### Tensorboard Stuff\n",
    "    # Writing the Graph\n",
    "    #tf.summary.scalar(\"accuracy\", accuracy)\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    \n",
    "    writer = tf.summary.FileWriter(save_path)\n",
    "    writer.add_graph(sess.graph)\n",
    "    \n",
    "    \n",
    "    # To merge all summary objects\n",
    "    \n",
    "    \n",
    "    \n",
    "    epoches = 1\n",
    "    for i in range(epoches):\n",
    "        for j,p in enumerate(paragraphs): \n",
    "            # Converting paragraphs to Embeddings\n",
    "            embeddings = dropout_layer(fast_text_model.inference(p)) # There should be the input placehodler\n",
    "            label = np.array(paragraphs_y[j]).reshape(1,-1)\n",
    "            print(\"________\")\n",
    "            print(embeddings.shape)\n",
    "            print(label.shape)\n",
    "            a = sess.run(average, feed_dict={x_input:embeddings, y_true:label})\n",
    "            #print(average)\n",
    "            print(a.shape)\n",
    "            l = sess.run(logits, feed_dict={x_input:embeddings, y_true:label})\n",
    "            print(l)\n",
    "            s = sess.run(softmax_layer, feed_dict={x_input:embeddings, y_true:label})\n",
    "            print(s)\n",
    "            c = sess.run(cross_entropy, feed_dict={x_input:embeddings, y_true:label})\n",
    "            print(c)\n",
    "           \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "    \"\"\"\n",
    "    print('Currently on step {}'.format(i))\n",
    "    print('Accuracy is:')\n",
    "    # Test the Train Model\n",
    "    matches = tf.equal(tf.argmax(cross_entropy,1),tf.argmax(y_true,1))\n",
    "\n",
    "    acc = tf.reduce_mean(tf.cast(matches,tf.float32))\n",
    "\n",
    "    print(sess.run(acc,feed_dict={x:mnist.test.images,y_true:mnist.test.labels,hold_prob:1.0}))\n",
    "    print('\\n')\n",
    "    \"\"\"\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after Epoch: 0.0\n",
      "Accuracy after Epoch: 0.0\n",
      "Accuracy after Epoch: 1.0\n",
      "Accuracy after Epoch: 0.0\n",
      "Accuracy after Epoch: 0.0\n",
      "Accuracy after Epoch: 1.0\n",
      "Accuracy after Epoch: 0.0\n",
      "Accuracy after Epoch: 1.0\n",
      "Accuracy after Epoch: 0.0\n",
      "Accuracy after Epoch: 1.0\n",
      "model_saved!\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess: \n",
    "    sess.run(init)\n",
    "    \n",
    "    ### Tensorboard Stuff\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(save_path)\n",
    "    writer.add_graph(sess.graph)\n",
    "\n",
    "    epoches = 10\n",
    "    for i in range(epoches):\n",
    "        for j,p in enumerate(paragraphs): \n",
    "            # Converting paragraphs to Embeddings\n",
    "            embeddings = dropout_layer(fast_text_model.inference(p))\n",
    "            label = np.array(paragraphs_y[j]).reshape(1,-1)\n",
    "            [_, train_accuracy, summary] = sess.run([train, accuracy, merged_summary], feed_dict={x_input:embeddings, y_true:label})\n",
    "            writer.add_summary(summary = summary, global_step = i)\n",
    "            writer.flush()\n",
    "            #[train_accurcy, summary] = sess.run([accuracy, merged_summary], feed_dict={x_input:embeddings, y_true:label})\n",
    "        print(\"Accuracy after Epoch: {}\".format(train_accuracy))\n",
    "    \n",
    "    saver.save(sess, os.path.join(save_path, model_name))\n",
    "    print(\"model_saved!\")\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from dan_checkpoints/first_dan.ckpt\n",
      "[[-0.16240382  0.39182154  0.06377319]]\n",
      "[[ 0.03230377 -0.17810996  0.03260929 -0.14536984  0.29796564 -0.46223411\n",
      "  -0.41599438  0.11585891  0.28831632  0.06492229  0.46638464  0.18014322\n",
      "   0.12224294 -0.19163228  0.03007289  0.33980093  0.00281868  0.0332146\n",
      "  -0.19013504 -0.16582564 -0.59048649 -0.14795333 -0.17244184  0.26569011\n",
      "  -0.34479357 -0.1828127  -0.03142293 -0.20770107 -0.44474537 -0.00919966\n",
      "   0.27196745 -0.07764005 -0.32889859  0.28723646  0.18341187  0.10273447\n",
      "  -0.31061691 -0.02057754  0.10167686 -0.20434808  0.24811438 -0.25169096\n",
      "  -0.06163547  0.00944988  0.3205502  -0.26943998 -0.41279809 -0.12671689\n",
      "  -0.0460205   0.04608156  0.18463844  0.33069187 -0.19325399  0.26622016\n",
      "  -0.37414399 -0.05859147 -0.28456428 -0.1796232  -0.44216661 -0.22569869\n",
      "  -0.62489675  0.04729576  0.07861766  0.42960647  0.21874051  0.27542228\n",
      "  -0.08337146  0.09598617 -0.2307338  -0.4421464  -0.20442049  0.25155391\n",
      "   0.00376899 -0.1643866   0.28297139 -0.07486268 -0.41758532 -0.15568619\n",
      "   0.11399986  0.20909046  0.29606682  0.12950223 -0.13017003 -0.0277605\n",
      "  -0.14145849  0.18508128  0.10178883 -0.03814424  0.39844673 -0.00792558\n",
      "  -0.25930127  0.14593004  0.31227447 -0.34317284  0.17003881  0.36488716\n",
      "   0.53281218 -0.035021   -0.00598956 -0.18820474]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess: \n",
    "    saver.restore(sess,os.path.join(save_path, model_name))\n",
    "    \n",
    "    # Is it predicting right? \n",
    "    embeddings = fast_text_model.inference([\"this\", \"sucks\"])\n",
    "    print(sess.run(logits, feed_dict={x_input:embeddings}))\n",
    "    \n",
    "    #Getting new Embeddings\n",
    "    print(sess.run(dense_layer_2, feed_dict={x_input:embeddings}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
