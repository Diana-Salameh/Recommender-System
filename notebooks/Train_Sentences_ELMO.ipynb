{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training using ELMO on Sentence Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Information & Label Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_name = \"test_elmo\" # Name of the model is at the same time the dir where model files will be stored\n",
    "classifier_dir = os.path.abspath(classifier_name)\n",
    "# label_map = \"\" # path to label map (not implemented yet..)\n",
    "data_dir = \"../data/datasets/daniel_0212\" # Where text data are stored\n",
    "test_dir = \"../data/eval_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model dir created!\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir(classifier_dir):\n",
    "    os.mkdir(classifier_dir)\n",
    "    print(\"model dir created!\")\n",
    "else:\n",
    "    print(\"model dir already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model (Classifier) & Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "training_steps = 1\n",
    "batch_size = 10\n",
    "\n",
    "# Model Parameters\n",
    "num_classes = 4 # ADJUST THAT\n",
    "hidden_units = [1024,512,256,64] # Iterable of number hidden units per layer. \n",
    "activation_fn=tf.nn.relu\n",
    "optimizer = tf.train.AdadeltaOptimizer(learning_rate=0.001)\n",
    "dropout = None # None or float 0-1\n",
    "batch_normalization = False\n",
    "regularization = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the label_map class from you package..\n",
    "def get_label_id(class_name:str):\n",
    "    if class_name == \"clustering\":   \n",
    "        return 0\n",
    "    if class_name == \"association\":\n",
    "        return 1\n",
    "    if class_name == \"regression\":\n",
    "        return 2\n",
    "    if class_name == \"classification\":\n",
    "        return 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment for TF-HUB Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce logging output.\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I dont know if he overwrites the model during retraining.\n",
    "embedding_model_url = \"https://tfhub.dev/google/elmo/2\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c95bf79028a6eec3f0bfd61ddb866cb183cde303\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "# The path where tf-hub will cache the model (use an absolute path..) \n",
    "os.environ[\"TFHUB_CACHE_DIR\"] = os.path.join(classifier_dir, \"embedding_model\")\n",
    "\n",
    "#TF-hub will store the name as hex\n",
    "embedding_model_hash = hashlib.sha1(classifier_dir.encode(\"utf8\")).hexdigest()\n",
    "print(embedding_model_hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data into Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(_dir:str, sentence_level = True):\n",
    "    data = {}\n",
    "    data[\"text\"] = []\n",
    "    data[\"class\"] = []\n",
    "    for root, dirs, files in os.walk(_dir):\n",
    "        for _dir in dirs: \n",
    "            for txt_file in [x for x in os.listdir(os.path.join(root, _dir)) if x.endswith((\".txt\", \".TXT\"))]:\n",
    "                # Class name = dir name\n",
    "                class_name = _dir\n",
    "                #Read File\n",
    "                file_name = os.path.join(root, _dir, txt_file)\n",
    "                file = open(file_name, \"r\")\n",
    "                txt = file.read()\n",
    "                file.close()\n",
    "                if sentence_level:\n",
    "                    # Txt to List[Sentences]\n",
    "                    sentences = sent_tokenize(txt)\n",
    "                    # Abstracts\n",
    "                    for sentence in sentences:\n",
    "                        data[\"text\"].append(sentence)\n",
    "                        data[\"class\"].append(get_label_id(class_name))\n",
    "                else:\n",
    "                    data[\"text\"].append(sentence)\n",
    "                    data[\"class\"].append(get_label_id(class_name))\n",
    "    df = pd.DataFrame.from_dict(data)\n",
    "    del data\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.19 s, sys: 780 ms, total: 3.97 s\n",
      "Wall time: 4.91 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = read_data(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5527</th>\n",
       "      <td>Building on the material of Part\\none of this ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13058</th>\n",
       "      <td>Recently the deep learning techniques have ach...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17019</th>\n",
       "      <td>A 2D laser\\ntriangulation scanner was selected...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33132</th>\n",
       "      <td>In this paper, we reveal that the problem is i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16876</th>\n",
       "      <td>This\\napproach can be applied progressively to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  class\n",
       "5527   Building on the material of Part\\none of this ...      3\n",
       "13058  Recently the deep learning techniques have ach...      3\n",
       "17019  A 2D laser\\ntriangulation scanner was selected...      1\n",
       "33132  In this paper, we reveal that the problem is i...      0\n",
       "16876  This\\napproach can be applied progressively to...      1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(frac=1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 37515 entries, 0 to 37514\n",
      "Data columns (total 2 columns):\n",
      "text     37515 non-null object\n",
      "class    37515 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 586.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = shuffle(df) # Shuffle the DataFrame\n",
    "X, Y = train_test_split(df, test_size=0.2, random_state = 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>We present an algorithm for randomized model g...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25642</th>\n",
       "      <td>(ii) New types of\\ndata are added and trained on.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25136</th>\n",
       "      <td>At the same time, the\\nlocality-aware query op...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  class\n",
       "611    We present an algorithm for randomized model g...      3\n",
       "25642  (ii) New types of\\ndata are added and trained on.      1\n",
       "25136  At the same time, the\\nlocality-aware query op...      1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19176</th>\n",
       "      <td>We exploit the effective application\\nlayer se...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33039</th>\n",
       "      <td>While these methods have been applied to both ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28632</th>\n",
       "      <td>In comparison with\\nperfect secret sharing it ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  class\n",
       "19176  We exploit the effective application\\nlayer se...      1\n",
       "33039  While these methods have been applied to both ...      0\n",
       "28632  In comparison with\\nperfect secret sharing it ...      1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data: \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 30012 entries, 611 to 18425\n",
      "Data columns (total 2 columns):\n",
      "text     30012 non-null object\n",
      "class    30012 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 703.4+ KB\n",
      "None\n",
      "\n",
      " Test Data: \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7503 entries, 19176 to 5319\n",
      "Data columns (total 2 columns):\n",
      "text     7503 non-null object\n",
      "class    7503 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 175.9+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Data: \\n\")\n",
    "print(X.info())\n",
    "print(\"\\n Test Data: \\n\")\n",
    "print(Y.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing Feature Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using /Users/Daniel/PycharmProjects/Recommender-System/notebooks/test_elmo/embedding_model to cache modules.\n",
      "INFO:tensorflow:Downloading TF-Hub Module 'https://tfhub.dev/google/elmo/2'.\n",
      "INFO:tensorflow:Downloading https://tfhub.dev/google/elmo/2: 20.35MB\n",
      "INFO:tensorflow:Downloading https://tfhub.dev/google/elmo/2: 40.35MB\n",
      "INFO:tensorflow:Downloading https://tfhub.dev/google/elmo/2: 60.35MB\n",
      "INFO:tensorflow:Downloading https://tfhub.dev/google/elmo/2: 80.35MB\n",
      "INFO:tensorflow:Downloading https://tfhub.dev/google/elmo/2: 100.35MB\n",
      "INFO:tensorflow:Downloading https://tfhub.dev/google/elmo/2: 110.35MB\n",
      "INFO:tensorflow:Downloading https://tfhub.dev/google/elmo/2: 130.35MB\n",
      "INFO:tensorflow:Downloading https://tfhub.dev/google/elmo/2: 150.35MB\n",
      "INFO:tensorflow:Downloading https://tfhub.dev/google/elmo/2: 170.35MB\n",
      "INFO:tensorflow:Downloading https://tfhub.dev/google/elmo/2: 190.35MB\n",
      "INFO:tensorflow:Downloading https://tfhub.dev/google/elmo/2: 210.35MB\n",
      "INFO:tensorflow:Downloading https://tfhub.dev/google/elmo/2: 230.35MB\n",
      "INFO:tensorflow:Downloading https://tfhub.dev/google/elmo/2: 250.35MB\n",
      "INFO:tensorflow:Downloading https://tfhub.dev/google/elmo/2: 270.35MB\n",
      "INFO:tensorflow:Downloading https://tfhub.dev/google/elmo/2: 290.35MB\n",
      "INFO:tensorflow:Downloading https://tfhub.dev/google/elmo/2: 310.35MB\n",
      "INFO:tensorflow:Downloading https://tfhub.dev/google/elmo/2: 330.35MB\n",
      "INFO:tensorflow:Downloading https://tfhub.dev/google/elmo/2: 350.35MB\n",
      "INFO:tensorflow:Downloaded https://tfhub.dev/google/elmo/2, Total size: 357.40MB\n",
      "INFO:tensorflow:Downloaded TF-Hub Module 'https://tfhub.dev/google/elmo/2'.\n",
      "CPU times: user 1min 22s, sys: 1min 37s, total: 3min\n",
      "Wall time: 7min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "embedded_text_feature_column = hub.text_embedding_column(\n",
    "    key=\"text\", \n",
    "    module_spec=embedding_model_url,\n",
    "    trainable = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR) # Reduce the stupid tf-warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = tf.estimator.DNNClassifier(\n",
    "    hidden_units= hidden_units,\n",
    "    feature_columns=[embedded_text_feature_column],\n",
    "    model_dir = classifier_dir,\n",
    "    activation_fn=activation_fn,\n",
    "    n_classes=num_classes,\n",
    "    dropout = dropout,\n",
    "    batch_norm=batch_normalization,\n",
    "    optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Input Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training input on the whole training set with no limit on training epochs.\n",
    "train_input_fn = tf.estimator.inputs.pandas_input_fn(\n",
    "    X, \n",
    "    X[\"class\"],\n",
    "    num_epochs=None,\n",
    "    batch_size= batch_size,\n",
    "    shuffle = False)\n",
    "\n",
    "# Prediction on the whole training set.\n",
    "predict_train_input_fn = tf.estimator.inputs.pandas_input_fn(\n",
    "    X, X[\"class\"], shuffle=False)\n",
    "\n",
    "# Prediction on the test set.\n",
    "predict_test_input_fn = tf.estimator.inputs.pandas_input_fn(\n",
    "    Y, Y[\"class\"], shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of the Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/envs/rs/lib/python3.6/site-packages/tensorflow/python/estimator/inputs/queues/feeding_queue_runner.py:62: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /anaconda3/envs/rs/lib/python3.6/site-packages/tensorflow/python/estimator/inputs/queues/feeding_functions.py:500: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "WARNING:tensorflow:From /anaconda3/envs/rs/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py:804: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /Users/Daniel/PycharmProjects/Recommender-System/notebooks/test_elmo/model.ckpt.\n",
      "INFO:tensorflow:loss = 13.454466, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /Users/Daniel/PycharmProjects/Recommender-System/notebooks/test_elmo/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 13.454466.\n",
      "CPU times: user 24.6 s, sys: 5.6 s, total: 30.2 s\n",
      "Wall time: 18.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "estimator.train(input_fn=train_input_fn, steps=training_steps);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(estimator, input_fn):\n",
    "    return [x[\"class_ids\"][0] for x in estimator.predict(input_fn=input_fn)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"clustering\", \"association\", \"regression\", \"classification\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = estimator.evaluate(input_fn=predict_train_input_fn)\n",
    "\n",
    "print(\"Results: \\n\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():\n",
    "    cm = tf.confusion_matrix(X[\"class\"], get_predictions(estimator, predict_train_input_fn))\n",
    "    with tf.Session() as session:\n",
    "        cm_out = session.run(cm)\n",
    "\n",
    "# Normalize the confusion matrix so that each row sums to 1.\n",
    "cm_out = cm_out.astype(float) / cm_out.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "sns.heatmap(cm_out, annot=True, xticklabels=labels, yticklabels=labels);\n",
    "plt.xlabel(\"Predicted\");\n",
    "plt.ylabel(\"True\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = estimator.evaluate(input_fn=predict_test_input_fn)\n",
    "\n",
    "print(\"Results: \\n\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():\n",
    "    cm = tf.confusion_matrix(Y[\"class\"], get_predictions(estimator, predict_test_input_fn))\n",
    "    with tf.Session() as session:\n",
    "        cm_out = session.run(cm)\n",
    "\n",
    "# Normalize the confusion matrix so that each row sums to 1.\n",
    "cm_out = cm_out.astype(float) / cm_out.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "sns.heatmap(cm_out, annot=True, xticklabels=labels, yticklabels=labels);\n",
    "plt.xlabel(\"Predicted\");\n",
    "plt.ylabel(\"True\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = read_data(test_dir, sentence_level = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle(df_test.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Input Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on the whole training set.\n",
    "predict_test_input_fn = tf.estimator.inputs.pandas_input_fn(\n",
    "    df_test, df_test[\"class\"], shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation of Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = estimator.evaluate(input_fn=predict_test_input_fn)\n",
    "\n",
    "print(\"Results: \\n\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():\n",
    "    cm = tf.confusion_matrix(df_test[\"class\"], get_predictions(estimator, predict_test_input_fn))\n",
    "    with tf.Session() as session:\n",
    "        cm_out = session.run(cm)\n",
    "\n",
    "# Normalize the confusion matrix so that each row sums to 1.\n",
    "cm_out = cm_out.astype(float) / cm_out.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "sns.heatmap(cm_out, annot=True, xticklabels=labels, yticklabels=labels);\n",
    "plt.xlabel(\"Predicted\");\n",
    "plt.ylabel(\"True\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Textual Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function and Sentences for Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_similarity(labels, features, rotation):\n",
    "    corr = np.inner(features, features)\n",
    "    sns.set(font_scale=1.2)\n",
    "    g = sns.heatmap(\n",
    "      corr,\n",
    "      xticklabels=labels,\n",
    "      yticklabels=labels,\n",
    "      vmin=0,\n",
    "      vmax=1,\n",
    "      cmap=\"YlOrRd\")\n",
    "    g.set_xticklabels(labels, rotation=rotation)\n",
    "    g.set_title(\"Semantic Textual Similarity\")\n",
    "\n",
    "\n",
    "def run_and_plot(session_, input_tensor_, messages_, encoding_tensor):\n",
    "    message_embeddings_ = session_.run(\n",
    "      encoding_tensor, feed_dict={input_tensor_: messages_})\n",
    "    plot_similarity(messages_, message_embeddings_, 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_0 = \"finding similar groups\"\n",
    "clustering_1 = \"clustering things\"\n",
    "pattern_mining_0 = \"analysing sequences\"\n",
    "pattern_mining_1 = \"finding pattern\"\n",
    "classification_0 = \"predicting categorical types\"\n",
    "classification_1 = \"classifing objects\"\n",
    "regression_0 = \"predicting prices\"\n",
    "regression_1 = \"forecasting numbers\"\n",
    "\n",
    "messages = [clustering_0,\n",
    "            clustering_1,\n",
    "            pattern_mining_0,\n",
    "            pattern_mining_1,\n",
    "            classification_0,\n",
    "            classification_1,\n",
    "            regression_0,\n",
    "            regression_1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Initial download takes a while till the model is downloaded from tf-hub (~1GB)\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "model = hub.Module(embedding_model_url, trainable = False) # trainable = True for Transfer Learning!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_input_placeholder = tf.placeholder(tf.string, shape=(None))\n",
    "similarity_message_encodings = model(similarity_input_placeholder)\n",
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    session.run(tf.tables_initializer())\n",
    "    \n",
    "    run_and_plot(session, similarity_input_placeholder, messages,\n",
    "               similarity_message_encodings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference for each Test Data File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inference = pd.DataFrame.from_dict({\"text\": df_test[\"text\"]})\n",
    "inference_func = tf.estimator.inputs.pandas_input_fn(df_inference, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = estimator.predict(inference_func)\n",
    "y = []\n",
    "probs =[]\n",
    "for x in results:\n",
    "    y.append(x[\"class_ids\"])\n",
    "    probs.append(x[\"probabilities\"])\n",
    "df_test[\"predicted\"] = y\n",
    "df_test[\"probs\"] = probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "220px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
