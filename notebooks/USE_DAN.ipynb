{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification - Universal Sentence Encoder\n",
    "\n",
    "In this notebook we will train the USE (DAN) on: \n",
    "\n",
    "1. The whole abstract\n",
    "2. Single Sentences of the whole abstract\n",
    "\n",
    "The data are not preprocessed. The removal of punctuations, etc. is done by the feature column module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I dont know if he overwrites the model during retraining.\n",
    "model_url = \"https://tfhub.dev/google/universal-sentence-encoder/2\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hashlib\n",
    "# The path where tf-hub will cache the model (use an absolute path..) \n",
    "os.environ[\"TFHUB_CACHE_DIR\"] = ''\n",
    "\n",
    "#TF-hub will store the name as hex\n",
    "hashlib.sha1(model_url.encode(\"utf8\")).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce logging output.\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.72 s, sys: 204 ms, total: 2.92 s\n",
      "Wall time: 2.96 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Initial download takes a while till the model is downloaded from tf-hub (~1GB)\n",
    "model = hub.Module(model_url, trainable = True) # trainable = True for Transfer Learning!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Abstract into a Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../../data/datasets/daniel_0212\"\n",
    "num_classes = 4 # ADJUST THAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADJUST THAT\n",
    "def get_label_id(class_name:str):\n",
    "    if class_name == \"clustering\":   \n",
    "        return 0\n",
    "    if class_name == \"association\":\n",
    "        return 1\n",
    "    if class_name == \"regression\":\n",
    "        return 2\n",
    "    if class_name == \"classification\":\n",
    "        return 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.57 s, sys: 327 ms, total: 3.89 s\n",
      "Wall time: 3.91 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_abstract = {}\n",
    "data_abstract[\"sentence\"] = []\n",
    "data_abstract[\"class\"] = []\n",
    "\n",
    "data_sentences = {}\n",
    "data_sentences[\"sentence\"] = []\n",
    "data_sentences[\"class\"] = []\n",
    "\n",
    "for root, dirs, files in os.walk(data_dir):\n",
    "    for _dir in dirs: \n",
    "        for txt_file in [x for x in os.listdir(os.path.join(root, _dir)) if x.endswith((\".txt\", \".TXT\"))]:\n",
    "            # Class name = dir name\n",
    "            class_name = _dir\n",
    "            #Read File\n",
    "            file_name = os.path.join(root, _dir, txt_file)\n",
    "            file = open(file_name, \"r\")\n",
    "            txt = file.read()\n",
    "            file.close()\n",
    "            # Abstractss\n",
    "            data_abstract[\"sentence\"].append(txt)\n",
    "            data_abstract[\"class\"].append(get_label_id(class_name))\n",
    "            # Sentences\n",
    "            sentences = sent_tokenize(txt)\n",
    "            for sentence in sentences:\n",
    "                data_sentences[\"sentence\"].append(sentence)\n",
    "                data_sentences[\"class\"].append(get_label_id(class_name))\n",
    "                \n",
    "            \n",
    "            \n",
    "            \n",
    "df_abstracts = pd.DataFrame.from_dict(data_abstract)\n",
    "df_sentences = pd.DataFrame.from_dict(data_sentences)\n",
    "del data_abstract\n",
    "del data_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5410</th>\n",
       "      <td>Clustering is an essential data mining tool th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3713</th>\n",
       "      <td>Single-cell RNA sequencing (scRNA-seq) is a fa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6603</th>\n",
       "      <td>Tensor regression has shown to be advantageous...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7678</th>\n",
       "      <td>Having a regression model, we are interested i...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5750</th>\n",
       "      <td>Subspace clustering is the problem of partitio...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  class\n",
       "5410  Clustering is an essential data mining tool th...      0\n",
       "3713  Single-cell RNA sequencing (scRNA-seq) is a fa...      1\n",
       "6603  Tensor regression has shown to be advantageous...      2\n",
       "7678  Having a regression model, we are interested i...      2\n",
       "5750  Subspace clustering is the problem of partitio...      0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_abstracts.sample(frac=1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8240 entries, 0 to 8239\n",
      "Data columns (total 2 columns):\n",
      "sentence    8240 non-null object\n",
      "class       8240 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 128.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df_abstracts.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45468</th>\n",
       "      <td>HDDC is based on the idea that high-dimensiona...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52189</th>\n",
       "      <td>We apply our method on a real example to estim...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47657</th>\n",
       "      <td>Spatial Auto-Regression (SAR) is a\\ncommon too...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38030</th>\n",
       "      <td>Some common clustering algorithms are\\napplied...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55678</th>\n",
       "      <td>We develop a novel \"decouple-recouple\" dynamic...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  class\n",
       "45468  HDDC is based on the idea that high-dimensiona...      0\n",
       "52189  We apply our method on a real example to estim...      2\n",
       "47657  Spatial Auto-Regression (SAR) is a\\ncommon too...      2\n",
       "38030  Some common clustering algorithms are\\napplied...      0\n",
       "55678  We develop a novel \"decouple-recouple\" dynamic...      2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentences.sample(frac=1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 58243 entries, 0 to 58242\n",
      "Data columns (total 2 columns):\n",
      "sentence    58243 non-null object\n",
      "class       58243 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 910.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df_sentences.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_abstracts = shuffle(df_abstracts) # Shuffle the DataFrame\n",
    "X_abstracts, Y_abstracts = train_test_split(df_abstracts, test_size=0.2, random_state = 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data: \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 6592 entries, 4127 to 5946\n",
      "Data columns (total 2 columns):\n",
      "sentence    6592 non-null object\n",
      "class       6592 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 154.5+ KB\n",
      "None\n",
      "\n",
      " Test Data: \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1648 entries, 7507 to 8158\n",
      "Data columns (total 2 columns):\n",
      "sentence    1648 non-null object\n",
      "class       1648 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 38.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Data: \\n\")\n",
    "print(X_abstracts.info())\n",
    "print(\"\\n Test Data: \\n\")\n",
    "print(Y_abstracts.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentences = shuffle(df_sentences) # Shuffle the DataFrame\n",
    "X_sentences, Y_sentences = train_test_split(df_sentences, test_size=0.2, random_state = 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data: \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 46594 entries, 7286 to 42326\n",
      "Data columns (total 2 columns):\n",
      "sentence    46594 non-null object\n",
      "class       46594 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 1.1+ MB\n",
      "None\n",
      "\n",
      " Test Data: \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 11649 entries, 8892 to 48762\n",
      "Data columns (total 2 columns):\n",
      "sentence    11649 non-null object\n",
      "class       11649 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 273.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Data: \\n\")\n",
    "print(X_sentences.info())\n",
    "print(\"\\n Test Data: \\n\")\n",
    "print(Y_sentences.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training of the Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Feature-Column Module\n",
    "\n",
    "1. Is responsible for the preprocessing of strings\n",
    "2. And constructs a dense representation of string (Embedding), which is feed into the classifier\n",
    "\n",
    "___\n",
    "```\n",
    "hub.text_embedding_column(\n",
    "    key,\n",
    "    module_spec,\n",
    "    trainable=False\n",
    ")\n",
    "```\n",
    "___\n",
    "\n",
    "- Key = The column in the DataFrame, which is passed to the Estimator\n",
    "- module_spec = The URL to the Embedding-Model\n",
    "- trainable = Retrain the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_text_feature_column = hub.text_embedding_column(\n",
    "    key=\"sentence\", \n",
    "    module_spec=model_url,\n",
    "    trainable = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_text_feature_column_retrain = hub.text_embedding_column(\n",
    "    key=\"sentence\", \n",
    "    module_spec=model_url,\n",
    "    trainable = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing the Estimator\n",
    "\n",
    "In the following we will just use an simple DNNClassifier. More: https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier\n",
    "\n",
    "\n",
    "\n",
    "Key Facts: \n",
    "\n",
    "0. We build two identical estimators, one for a retrained embedding model the other with transer learning. \n",
    "1. Optimizer Adadelta - seems to be the fastest (http://ruder.io/optimizing-gradient-descent/index.html#amsgrad) \n",
    "2. Hidden Units: [1024, 512, 256]. I dont know if it makes sense but before i tried less hidden units and i want to check for an improvement.\n",
    "3. No Dropout\n",
    "4. No Batch-Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO) # Reduce the stupid tf-warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp786y30g8\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp786y30g8', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fb140524ef0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "estimator_abstracts = tf.estimator.DNNClassifier(\n",
    "    hidden_units=[1024, 512, 256],\n",
    "    feature_columns=[embedded_text_feature_column],\n",
    "    model_dir = \"models_save/abstracts\",\n",
    "    n_classes=num_classes,\n",
    "    optimizer=tf.train.AdadeltaOptimizer(learning_rate=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_sentences = tf.estimator.DNNClassifier(\n",
    "    hidden_units=[1024, 512, 256],\n",
    "    feature_columns=[embedded_text_feature_column],\n",
    "    model_dir = \"models_save/sentences\",\n",
    "    n_classes=num_classes,\n",
    "    optimizer=tf.train.AdadeltaOptimizer(learning_rate=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_retrain_abstracts = tf.estimator.DNNClassifier(\n",
    "    hidden_units=[1024, 512, 256],\n",
    "    feature_columns=[embedded_text_feature_column_retrain],\n",
    "    model_dir = \"models_save/abstracts_retrain\",\n",
    "    n_classes=num_classes,\n",
    "    optimizer=tf.train.AdadeltaOptimizer(learning_rate=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_retrain_sentences = tf.estimator.DNNClassifier(\n",
    "    hidden_units=[1024, 512, 256],\n",
    "    feature_columns=[embedded_text_feature_column_retrain],\n",
    "    model_dir = \"models_save/sentences_retrain\",\n",
    "    n_classes=num_classes,\n",
    "    optimizer=tf.train.AdadeltaOptimizer(learning_rate=0.001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Input Functions\n",
    "\n",
    "Here we also don't play around with hyperparameters. Batch size is standard batch size of 128. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrame with Abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training input on the whole training set with no limit on training epochs.\n",
    "train_input_fn_abstracts = tf.estimator.inputs.pandas_input_fn(\n",
    "    X_abstracts, X_abstracts[\"class\"], num_epochs=None, shuffle = False)\n",
    "\n",
    "# Prediction on the whole training set.\n",
    "predict_train_input_fn_abstracts = tf.estimator.inputs.pandas_input_fn(\n",
    "    X_abstracts, X_abstracts[\"class\"], shuffle=False)\n",
    "# Prediction on the test set.\n",
    "predict_test_input_fn_abstracts = tf.estimator.inputs.pandas_input_fn(\n",
    "    Y_abstracts, Y_abstracts[\"class\"], shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrame with Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training input on the whole training set with no limit on training epochs.\n",
    "train_input_fn_sentences = tf.estimator.inputs.pandas_input_fn(\n",
    "    X_sentences, X_sentences[\"class\"], num_epochs=None, shuffle = False)\n",
    "\n",
    "# Prediction on the whole training set.\n",
    "predict_train_input_fn_sentences = tf.estimator.inputs.pandas_input_fn(\n",
    "    X_sentences, X_sentences[\"class\"], shuffle=False)\n",
    "# Prediction on the test set.\n",
    "predict_test_input_fn_sentences = tf.estimator.inputs.pandas_input_fn(\n",
    "    Y_sentences, Y_sentences[\"class\"], shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimators without retraining the embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmp786y30g8/model.ckpt-0\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmp786y30g8/model.ckpt.\n",
      "INFO:tensorflow:loss = 177.35513, step = 1\n",
      "INFO:tensorflow:global_step/sec: 5.66988\n",
      "INFO:tensorflow:loss = 46.401173, step = 101 (17.641 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.97983\n",
      "INFO:tensorflow:loss = 9.45266, step = 201 (16.722 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.94714\n",
      "INFO:tensorflow:loss = 11.922728, step = 301 (11.175 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.2158\n",
      "INFO:tensorflow:loss = 5.3924923, step = 401 (10.852 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.09024\n",
      "INFO:tensorflow:loss = 11.89921, step = 501 (16.422 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.99032\n",
      "INFO:tensorflow:loss = 9.882598, step = 601 (16.691 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.05516\n",
      "INFO:tensorflow:loss = 5.8314395, step = 701 (16.513 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.13847\n",
      "INFO:tensorflow:loss = 6.443844, step = 801 (16.291 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.09696\n",
      "INFO:tensorflow:loss = 6.900069, step = 901 (16.403 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.14388\n",
      "INFO:tensorflow:loss = 4.43777, step = 1001 (16.277 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.12529\n",
      "INFO:tensorflow:loss = 8.65637, step = 1101 (16.324 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.08675\n",
      "INFO:tensorflow:loss = 4.0819883, step = 1201 (16.433 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.75945\n",
      "INFO:tensorflow:loss = 9.020698, step = 1301 (17.361 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.04129\n",
      "INFO:tensorflow:loss = 4.20874, step = 1401 (16.551 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.9648\n",
      "INFO:tensorflow:loss = 2.446295, step = 1501 (16.766 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.0979\n",
      "INFO:tensorflow:loss = 4.1327534, step = 1601 (16.400 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.18055\n",
      "INFO:tensorflow:loss = 5.1311283, step = 1701 (16.179 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.12541\n",
      "INFO:tensorflow:loss = 7.495531, step = 1801 (16.325 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.18926\n",
      "INFO:tensorflow:loss = 6.212965, step = 1901 (16.158 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.19809\n",
      "INFO:tensorflow:loss = 6.7979503, step = 2001 (16.134 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.88348\n",
      "INFO:tensorflow:loss = 4.0636287, step = 2101 (16.997 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.06178\n",
      "INFO:tensorflow:loss = 6.1705894, step = 2201 (16.497 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.09726\n",
      "INFO:tensorflow:loss = 5.6508327, step = 2301 (16.401 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.1712\n",
      "INFO:tensorflow:loss = 7.3349934, step = 2401 (16.204 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.1379\n",
      "INFO:tensorflow:loss = 4.093986, step = 2501 (16.297 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.13336\n",
      "INFO:tensorflow:loss = 6.357033, step = 2601 (16.300 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.13178\n",
      "INFO:tensorflow:loss = 5.3107023, step = 2701 (16.310 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.11615\n",
      "INFO:tensorflow:loss = 8.111983, step = 2801 (16.348 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.08966\n",
      "INFO:tensorflow:loss = 2.5042057, step = 2901 (16.426 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.06418\n",
      "INFO:tensorflow:loss = 4.9017286, step = 3001 (16.490 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.09215\n",
      "INFO:tensorflow:loss = 4.381811, step = 3101 (16.412 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.9671\n",
      "INFO:tensorflow:loss = 2.9016585, step = 3201 (16.760 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.10011\n",
      "INFO:tensorflow:loss = 3.0389218, step = 3301 (16.392 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.09222\n",
      "INFO:tensorflow:loss = 3.9193387, step = 3401 (16.413 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.07138\n",
      "INFO:tensorflow:loss = 8.031986, step = 3501 (16.470 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.08841\n",
      "INFO:tensorflow:loss = 5.467658, step = 3601 (16.425 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3699 into /tmp/tmp786y30g8/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 4.53072\n",
      "INFO:tensorflow:loss = 7.024925, step = 3701 (22.084 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.07453\n",
      "INFO:tensorflow:loss = 4.775951, step = 3801 (16.454 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.33967\n",
      "INFO:tensorflow:loss = 5.3009086, step = 3901 (18.726 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4000 into /tmp/tmp786y30g8/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 6.0683994.\n"
     ]
    }
   ],
   "source": [
    "estimator_abstracts.train(input_fn=train_input_fn_abstracts, steps=4000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_sentences.train(input_fn=train_input_fn_sentences, steps=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimators with retraining the embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_retrain_abstracts.train(input_fn=train_input_fn_abstracts, steps=4000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_retrain_sentences.train(input_fn=train_input_fn_sentences, steps=4000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_abstracts = estimator_abstracts.evaluate(input_fn=predict_train_input_fn_abstracts)\n",
    "result_sentences = estimator_abstracts.evaluate(input_fn=predict_train_input_fn_sentences)\n",
    "\n",
    "print(\"Model learned with abstracts and without retraining: \\n\")\n",
    "print(result_abstracts)\n",
    "\n",
    "print(\"Model learned with sentences and without retraining: \\n\")\n",
    "print(result_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_abstracts = estimator_retrain_abstracts.evaluate(input_fn=predict_train_input_fn_abstracts)\n",
    "result_sentences = estimator_retrain_abstracts.evaluate(input_fn=predict_train_input_fn_sentences)\n",
    "\n",
    "print(\"Model learned with abstracts and retraining: \\n\")\n",
    "print(result_abstracts)\n",
    "\n",
    "print(\"Model learned with sentences and retraining: \\n\")\n",
    "print(result_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_abstracts = estimator_abstracts.evaluate(input_fn=predict_test_input_fn_abstracts)\n",
    "result_sentences = estimator_abstracts.evaluate(input_fn=predict_test_input_fn_sentences)\n",
    "\n",
    "print(\"Model learned with abstracts and without retraining: \\n\")\n",
    "print(result_abstracts)\n",
    "\n",
    "print(\"Model learned with sentences and without retraining: \\n\")\n",
    "print(result_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-06-17:43:29\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmp786y30g8/model.ckpt-4000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-06-17:43:55\n",
      "INFO:tensorflow:Saving dict for global step 4000: accuracy = 0.9764866, average_loss = 0.033985835, global_step = 4000, loss = 4.308358\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 4000: /tmp/tmp786y30g8/model.ckpt-4000\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-06-17:44:07\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmp786y30g8/model.ckpt-4000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-06-17:44:27\n",
      "INFO:tensorflow:Saving dict for global step 4000: accuracy = 0.8246359, average_loss = 0.8390833, global_step = 4000, loss = 106.36995\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 4000: /tmp/tmp786y30g8/model.ckpt-4000\n",
      "Training set accuracy: 0.9764866232872009\n",
      "Test set accuracy: 0.824635922908783\n"
     ]
    }
   ],
   "source": [
    "result_abstracts_retrain = estimator_retrain_abstracts.evaluate(input_fn=predict_test_input_fn_abstracts)\n",
    "result_sentences_retrain = estimator_retrain_abstracts.evaluate(input_fn=predict_test_input_fn_sentences)\n",
    "\n",
    "print(\"Model learned with abstracts and without retraining: \\n\")\n",
    "print(result_abstracts)\n",
    "\n",
    "print(\"Model learned with sentences and without retraining: \\n\")\n",
    "print(result_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TestSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
